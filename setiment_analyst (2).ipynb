{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gG_bSpzuReXf"
   },
   "source": [
    "# Phần 1: Thư viện"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F1tK00OAReXg"
   },
   "source": [
    "- **re**: Xử lý chuỗi với các biểu thức chính quy\n",
    "- **demoji**: Biểu tượng cảm xúc\n",
    "- **pickle**: Lưu trữ và truy xuất dữ liệu\n",
    "- **numpy**: Tính toán\n",
    "- **pandas**: Cấu trúc dữ liệu và công cụ phân tích dữ liệu\n",
    "- **matplotlib.pyplot**: Biểu đồ trong Python\n",
    "- **tensorflow**: Xây dựng và huấn luyện mô hình học máy\n",
    "- **scikit-learn**: Các công cụ cho việc học máy và thống kê\n",
    "- **pyvi**: Thư viện tiếng Việt hóa văn bản"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "usrduxzlReXh",
    "outputId": "a752e094-b583-461d-ebb8-d7f30d8313e9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_15220\\2974295818.py:21: FutureWarning: The demoji.download_codes attribute is deprecated and will be removed from demoji in a future version. It is an unused attribute as emoji codes are now distributed directly with the demoji package.\n",
      "  demoji.download_codes()\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import demoji\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, Dense, Dropout, Bidirectional, LSTM, GRU, Input, GlobalMaxPooling1D, LayerNormalization, Conv1D, MaxPooling1D, ELU\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from pyvi import ViTokenizer, ViUtils\n",
    "\n",
    "\n",
    "demoji.download_codes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6kLKp7cBReXi"
   },
   "source": [
    "# Phần 2: Xử lý dữ liệu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R3daWXINReXi"
   },
   "source": [
    "Nhập liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 498
    },
    "id": "ldYBhmgaReXj",
    "outputId": "45aad754-261b-47ca-beb1-ed183d2247ab"
   },
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 2 fields in line 3, saw 3\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata_modified.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m missing_values \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39misnull()\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSố lượng hàng: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    610\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 611\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1778\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1771\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m     (\n\u001b[0;32m   1775\u001b[0m         index,\n\u001b[0;32m   1776\u001b[0m         columns,\n\u001b[0;32m   1777\u001b[0m         col_dict,\n\u001b[1;32m-> 1778\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1779\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1780\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1782\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:230\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 230\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    232\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:808\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:866\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:852\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1973\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 2 fields in line 3, saw 3\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('data_modified.csv')\n",
    "missing_values = data.isnull().sum()\n",
    "\n",
    "\n",
    "print(f\"Số lượng hàng: {data.shape[0]}\")\n",
    "print(f\"Số lượng cột: {data.shape[1]}\")\n",
    "print(f\"Tên cột: {list(data.columns)}\")\n",
    "print(f\"Thông tin cột bị thiếu: \\n{missing_values}\")\n",
    "data.info()\n",
    "\n",
    "sentiment_data = data\n",
    "sentiment_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "DI8MbLf0ReXj",
    "outputId": "84e9cc4c-8799-4383-fa33-86d5b4eea130"
   },
   "outputs": [],
   "source": [
    "pos_data = sentiment_data[sentiment_data['label'] == 'POS']\n",
    "pos_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "sAKAECHpReXj",
    "outputId": "451904ec-2b5c-4aef-8031-86f34a72d01a"
   },
   "outputs": [],
   "source": [
    "neu_data = sentiment_data[sentiment_data['label'] == 'NEU']\n",
    "neu_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "I-5qQay-ReXj",
    "outputId": "81cb230f-19ff-45e2-e01e-6805ad3acc5f"
   },
   "outputs": [],
   "source": [
    "neg_data = sentiment_data[sentiment_data['label'] == 'NEG']\n",
    "neg_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e6ok0Wa5ReXk"
   },
   "source": [
    "Tiền xử lý"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zWuAR18IReXk"
   },
   "outputs": [],
   "source": [
    "def remove_emo(text):\n",
    "    emo = demoji.replace(text, '')\n",
    "    return emo\n",
    "\n",
    "def remove_urls(text):\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_mentions_and_emails(text):\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '', text)\n",
    "    return text\n",
    "\n",
    "def lower_case (text):\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uuOo8_mPReXk",
    "outputId": "1e70cfad-dd20-4ab7-d30d-36c1c6aeed3b"
   },
   "outputs": [],
   "source": [
    "input_data = sentiment_data['comment'].values\n",
    "input_label = sentiment_data['label'].values\n",
    "\n",
    "label_dict = {'NEG':0,'NEU':1,'POS':2}\n",
    "\n",
    "input_pre = []\n",
    "label_with_accent = []\n",
    "for idx, dt in enumerate(input_data):\n",
    "    #Chuyển đổi thành list\n",
    "    input_text_pre = list(tf.keras.preprocessing.text.text_to_word_sequence(dt))\n",
    "    # Chuyển danh sách từ 1 chuỗi\n",
    "    input_text_pre = \" \".join(input_text_pre)\n",
    "\n",
    "    # Tiền xử lý dữ liệu\n",
    "    input_text_pre = remove_emo(input_text_pre)\n",
    "    input_text_pre = remove_urls(input_text_pre)\n",
    "    input_text_pre = remove_mentions_and_emails(input_text_pre)\n",
    "    input_text_pre = lower_case(input_text_pre)\n",
    "\n",
    "    # Tách dấu\n",
    "    input_text_pre_no_accent = str(ViUtils.remove_accents(input_text_pre).decode(\"utf-8\"))\n",
    "\n",
    "    # Tách từ\n",
    "    input_text_pre_accent = ViTokenizer.tokenize(input_text_pre)\n",
    "    input_text_pre_no_accent = ViTokenizer.tokenize(input_text_pre_no_accent)\n",
    "\n",
    "    input_pre.append(input_text_pre_accent)\n",
    "    input_pre.append(input_text_pre_no_accent)\n",
    "    label_with_accent.append(input_label[idx])\n",
    "    label_with_accent.append(input_label[idx])\n",
    "\n",
    "print(len(input_pre))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ivnUPtWoReXk"
   },
   "source": [
    "Biểu đồ histogram dữ liệu từ 0 đến 31k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ZDTr9rWFReXl",
    "outputId": "88fe0fc4-8800-476c-d9d8-044bc36d47de"
   },
   "outputs": [],
   "source": [
    "bin_count = 10  # Số lượng bin\n",
    "chunk_size = 1000   # Kích thước của mỗi phân đoạn\n",
    "total_chunks = 310000 // chunk_size      # Tổng số phân đoạn\n",
    "\n",
    "# Tạo 1 figure mới\n",
    "fig, axes = plt.subplots(nrows=(total_chunks // 6) + (1 if total_chunks % 6 != 0 else 0), ncols=6, figsize=(20, (total_chunks // 6) * 5))\n",
    "\n",
    "for i, ax in zip(range(0, 31000, chunk_size), axes.flatten()):\n",
    "    seq_len = [len(sentence.split()) for sentence in input_pre[i:i+chunk_size]]     # Tính độ dài của từng câu\n",
    "    pd.Series(seq_len).hist(bins=bin_count, ax=ax)      # Vẽ histogram\n",
    "    ax.set_title(f'{i} đến {i+chunk_size}')     # Tiêu đề\n",
    "\n",
    "# Xóa các trục con thừa\n",
    "for j in range(total_chunks, len(axes.flatten())):\n",
    "    fig.delaxes(axes.flatten()[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lwb-czz2ReXl",
    "outputId": "99f6c62e-8e09-47a8-d398-91daede9819f"
   },
   "outputs": [],
   "source": [
    "# chuyển đổi các nhãn từ dạng chuỗi sang dạng số\n",
    "label_idx = [label_dict[i] for i in label_with_accent]\n",
    "# mã hóa one-hot\n",
    "label_tf = tf.keras.utils.to_categorical(label_idx, num_classes=3)\n",
    "label_tf = label_tf.astype('float32')\n",
    "\n",
    "# xử lý văn bản thành các chuỗi số và đệm\n",
    "tokenizer_data = Tokenizer(oov_token='<OOV>', filters='', split=' ')    # tạo từu khóa OOV, không lọc ký tự cách từ = dấu cách\n",
    "tokenizer_data.fit_on_texts(input_pre)  # học từ điển từ danh sách đã tiền xử lý\n",
    "\n",
    "tokenizer_data_text = tokenizer_data.texts_to_sequences(input_pre)      # vb => chuỗi số\n",
    "vec_data = pad_sequences(tokenizer_data_text, padding='post', maxlen=512) # đệm chuỗi số sang dạng cố định 512\n",
    "\n",
    "#lưu tokenizer\n",
    "pickle.dump(tokenizer_data, open(\"tokenizer_data.pkl\",\"wb\"))\n",
    "\n",
    "print(f\"Kích thước dữ liệu đầu vào: {vec_data.shape}\")\n",
    "data_vocab_size = len(tokenizer_data.word_index)+1  # kích thước từ điển + từ khóa OOV\n",
    "print(f\"Kích thước của từ điển: {data_vocab_size}\")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(vec_data, label_tf, test_size=0.3, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.15, random_state=42)\n",
    "\n",
    "\n",
    "# Áp dụng SMOTE để oversample lớp thiểu số\n",
    "smote = SMOTE(sampling_strategy={1: len(y_train[y_train[:, 1] == 1]) * 2}, random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\"Số mẫu trong tập huấn luyện: {len(X_resampled)}\")\n",
    "print(f\"Số mẫu trong tập validation: {len(X_val)}\")\n",
    "print(f\"Số mẫu trong tập kiểm tra: {len(X_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8S1OHpvxCByQ",
    "outputId": "5c6ffc84-688c-4dee-85d5-ac580876476e"
   },
   "outputs": [],
   "source": [
    "y_train[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CrDCk5QVReXl"
   },
   "source": [
    "# Phần 3: Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C_0sC_DmReXl",
    "outputId": "d287339f-90af-4485-be4e-5b66557bf656"
   },
   "outputs": [],
   "source": [
    "def generate_model():\n",
    "    dropout_threshold = 0.5     # Ngưỡng dropout\n",
    "    input_dim = data_vocab_size     # Kích thước từ vựng\n",
    "    output_dim = 64    # Kích thước của vector embedding\n",
    "    input_length = 512  # Độ dài tối đa\n",
    "    initializer = tf.keras.initializers.GlorotNormal()\n",
    "\n",
    "    input_layer = Input(shape=(input_length,))\n",
    "    # chuyển đổi từ khóa vào vector\n",
    "    feature = Embedding(input_dim=input_dim, output_dim=output_dim, input_length=input_length, embeddings_initializer=initializer)(input_layer)\n",
    "\n",
    "    # CNN để trích xuất đặc trưng\n",
    "    cnn_feature = Conv1D(filters=64, kernel_size=3, padding='same', activation='relu')(feature)\n",
    "    cnn_feature = MaxPooling1D()(cnn_feature)\n",
    "    cnn_feature = Dropout(dropout_threshold)(cnn_feature)\n",
    "    cnn_feature = Conv1D(filters= 64, kernel_size=3, padding='same', activation='relu')(cnn_feature)\n",
    "    cnn_feature = MaxPooling1D()(cnn_feature)\n",
    "    cnn_feature = LayerNormalization()(cnn_feature)\n",
    "    cnn_feature = Dropout(dropout_threshold)(cnn_feature)\n",
    "\n",
    "    # Mạng Bi-directional LSTM để xử lý tuần tự dữ liệu\n",
    "    bi_lstm_feature = Bidirectional(LSTM(units= 64, dropout=dropout_threshold, return_sequences=True, kernel_initializer=initializer), merge_mode='concat')(feature)\n",
    "    bi_lstm_feature = MaxPooling1D()(bi_lstm_feature)\n",
    "    bi_lstm_feature = Bidirectional(LSTM(units=64, dropout=dropout_threshold, return_sequences=True, kernel_initializer=initializer), merge_mode='concat')(bi_lstm_feature)\n",
    "    bi_lstm_feature = MaxPooling1D()(bi_lstm_feature)\n",
    "\n",
    "    bi_lstm_feature = LayerNormalization()(bi_lstm_feature)\n",
    "\n",
    "    # Kết hợp đặc trưng từ CNN và Bi-LSTM\n",
    "    combine_feature = tf.keras.layers.Concatenate()([cnn_feature, bi_lstm_feature])\n",
    "    combine_feature = GlobalMaxPooling1D()(combine_feature)\n",
    "    combine_feature = LayerNormalization()(combine_feature)\n",
    "\n",
    "    # Các lớp Dense (MLP) để phân loại\n",
    "    classifier = Dense(90, activation='relu')(combine_feature)\n",
    "    classifier = Dropout(0.3)(classifier)\n",
    "    classifier = Dense(70, activation='relu')(classifier)\n",
    "    classifier = Dropout(0.3)(classifier)\n",
    "    classifier = Dense(50, activation='relu')(classifier)\n",
    "    classifier = Dropout(0.3)(classifier)\n",
    "    classifier = Dense(30, activation='relu')(classifier)\n",
    "    classifier = Dropout(0.3)(classifier)\n",
    "    classifier = Dense(10, activation='relu')(classifier)\n",
    "    classifier = Dropout(0.3)(classifier)\n",
    "    classifier = Dense(3, activation='softmax')(classifier)\n",
    "\n",
    "    # Tạo mô hình Keras\n",
    "    model = Model(inputs=input_layer, outputs=classifier)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = generate_model()\n",
    "adam = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oj_0p1bKReXm"
   },
   "source": [
    "Huấn luyện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ztq8fqPcReXm",
    "outputId": "94e0383c-77cf-44a6-cce2-df281944de02"
   },
   "outputs": [],
   "source": [
    "# Định nghĩa callback để lưu mô hình\n",
    "callback_model = tf.keras.callbacks.ModelCheckpoint(\n",
    "    'model_cnn_bilstm.keras',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True  # Chỉ lưu mô hình tốt nhất\n",
    ")\n",
    "\n",
    "\n",
    "# Huấn luyện mô hình\n",
    "history = model.fit(\n",
    "    x=X_resampled,\n",
    "    y=y_resampled,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=15,\n",
    "    batch_size=128,\n",
    "    callbacks=[callback_model]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_aaN9QQxReXm",
    "outputId": "9b609ed1-371b-4bd5-a4b0-d7dc334ee340"
   },
   "outputs": [],
   "source": [
    "model.load_weights(\"model_cnn_bilstm.keras\")\n",
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pYdUbHJdzBSk",
    "outputId": "a4396ed1-5645-4ac5-ccba-1363c67c6de9"
   },
   "outputs": [],
   "source": [
    "e = model.layers[1]\n",
    "weights = e.get_weights()[0]\n",
    "print(weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "80CAdgruzMHB"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
    "for word_num in range(1, data_vocab_size):\n",
    "    word = tokenizer_data.index_word[word_num]\n",
    "    embeddings = weights[word_num]\n",
    "    out_m.write(word + \"\\n\")\n",
    "    out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "y0jCrdBUReXm",
    "outputId": "b095a985-d55a-4ba6-ff53-47837a5a2cbf"
   },
   "outputs": [],
   "source": [
    "# Trực quan hóa hàm mất mát\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Hàm mất mát của tập train và tập validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Trực quan hóa độ chính xác\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Độ chính xác của tập train và tập validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pbQL2eGAReXm"
   },
   "source": [
    "# Phần 4: Chạy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TyWZC0PBReXm"
   },
   "outputs": [],
   "source": [
    "# Hàm tiền xử lý đầu vào văn bản th\n",
    "def preprocess_raw_input(raw_input, tokenizer):\n",
    "    input_text_pre = list(tf.keras.preprocessing.text.text_to_word_sequence(raw_input))\n",
    "    input_text_pre = \" \".join(input_text_pre)\n",
    "    input_text_pre = remove_emo(input_text_pre)\n",
    "    input_text_pre = remove_urls(input_text_pre)\n",
    "    input_text_pre = remove_mentions_and_emails(input_text_pre)\n",
    "    input_text_pre = lower_case(input_text_pre)\n",
    "    input_text_pre_accent = ViTokenizer.tokenize(input_text_pre)\n",
    "    print(f\"Comment: {input_text_pre_accent}\")\n",
    "    tokenizer_data_text = tokenizer.texts_to_sequences([input_text_pre_accent])\n",
    "    vec_data = pad_sequences(tokenizer_data_text, padding='post', maxlen=512)\n",
    "    return vec_data\n",
    "\n",
    "# Hàm này nhận đầu vào đã tiền xử lý và mô hình để thực hiện suy diễn và trả về kết quả dự đoán\n",
    "def inference_model(input_feature, model):\n",
    "    output = model(input_feature).numpy()[0]   # Dự đoán bằng mô hình\n",
    "    result = output.argmax()        # Lấy nhãn có xác suất cao nhất\n",
    "    conf = float(output.max())\n",
    "    label_dict = {'NEG': 0, 'NEU': 1, 'POS': 2}         # Từ điển các nhãn\n",
    "    label = list(label_dict.keys())\n",
    "    return label[int(result)], conf\n",
    "\n",
    "# Hàm  kết hợp tiền xử lý và suy diễn\n",
    "def prediction(raw_input, tokenizer, model):\n",
    "    input_model = preprocess_raw_input(raw_input, tokenizer)        # Tiền xử lý đầu vào\n",
    "    # Dự đoán kết quả\n",
    "    result, conf = inference_model(input_model, model)\n",
    "    return result, conf\n",
    "\n",
    "# Tạo và tải mô hình\n",
    "my_model = generate_model()\n",
    "my_model = load_model('model_cnn_bilstm.keras')\n",
    "\n",
    "# Tải tokenizer từ file\n",
    "with open(r\"tokenizer_data.pkl\", \"rb\") as input_file:\n",
    "    my_tokenizer = pickle.load(input_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZRxJb-XQReXn"
   },
   "outputs": [],
   "source": [
    "# Hàm để phân loại các bình luận từ file CSV\n",
    "def classify_comments_from_csv(file_path, tokenizer, model):\n",
    "    df = pd.read_csv(file_path)\n",
    "    comments = df['comment']  # Giả sử cột chứa bình luận có tên là 'comment'\n",
    "\n",
    "    results = []\n",
    "    for comment in comments:\n",
    "        result, conf = prediction(comment, tokenizer, model)\n",
    "        results.append((comment, result, conf))\n",
    "\n",
    "    result_df = pd.DataFrame(results, columns=['comment', 'label', 'confidence'])\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FTJqexiwReXn"
   },
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "from tkinter import filedialog\n",
    "from PIL import Image, ImageTk\n",
    "import pandas as pd\n",
    "\n",
    "history_list = []\n",
    "\n",
    "def on_predict():\n",
    "    user_input = input_text.get(\"1.0\", tk.END).strip()\n",
    "    if user_input:\n",
    "        result, confidence = prediction(user_input, my_tokenizer, my_model)\n",
    "        result_label.config(text=f\"Kết quả bình luận: {result}\")\n",
    "        save_history(user_input, result)\n",
    "\n",
    "def save_history(comment, result):\n",
    "    history_list.append((comment, result))\n",
    "\n",
    "def delete_history():\n",
    "    global history_list\n",
    "    history_list = []\n",
    "    show_history()\n",
    "\n",
    "def export_history():\n",
    "    if not history_list:\n",
    "        return\n",
    "    file_path = filedialog.asksaveasfilename(defaultextension='', filetypes=[('CSV files', '*.csv'), ('Text files', '*.txt'), ('Excel files', '*.xlsx')])\n",
    "    if file_path:\n",
    "        df = pd.DataFrame(history_list, columns=['Bình luận', 'Kết quả'])\n",
    "        if file_path.endswith('.csv'):\n",
    "            df.to_csv(file_path, index=False)\n",
    "        elif file_path.endswith('.txt'):\n",
    "            df.to_csv(file_path, index=False, sep=',')\n",
    "        elif file_path.endswith('.xlsx'):\n",
    "            df.to_excel(file_path, index=False)\n",
    "        else:\n",
    "            tk.messagebox.showerror('Lỗi', 'Định dạng tệp không được hỗ trợ')\n",
    "            return\n",
    "        tk.messagebox.showinfo('Lưu thành công', f'Dữ liệu đã được lưu vào {file_path}')\n",
    "\n",
    "def show_history():\n",
    "    history_window = tk.Toplevel(root)\n",
    "    history_window.title(\"Lịch sử\")\n",
    "    history_window.geometry(\"280x350\")\n",
    "    history_window.resizable(width=False, height=False)\n",
    "\n",
    "    if not history_list:\n",
    "        ttk.Label(history_window, text=\"Chưa có dữ liệu lịch sử.\").pack(padx=10, pady=10)\n",
    "    else:\n",
    "        frame_P = ttk.Frame(history_window, borderwidth=3, relief=\"solid\", width=280, height=280)\n",
    "        frame_P.pack(side='top', pady=5)\n",
    "        frame_P.pack_propagate(False)\n",
    "        frame_tree = ttk.Frame(frame_P, borderwidth=3, relief=\"flat\", width=280, height=256)\n",
    "        frame_tree.pack(side='top')\n",
    "        frame_tree.pack_propagate(False)\n",
    "        tree_frame = ttk.Frame(frame_tree, borderwidth=0, relief=\"flat\", width=250, height=256)\n",
    "        tree_frame.pack(side=\"left\", expand=True)\n",
    "        tree_frame.pack_propagate(False)\n",
    "        text_widget = tk.Text(tree_frame, wrap='word', width=42, height=17)\n",
    "        text_widget.pack(side='top',expand=True)\n",
    "        scrollbar_y = tk.Scrollbar(frame_tree, orient=\"vertical\")\n",
    "        scrollbar_y.pack(side=\"right\", fill=\"y\")\n",
    "        scrollbar_x = tk.Scrollbar(frame_P, orient=\"horizontal\")\n",
    "        scrollbar_x.pack(side=\"top\", fill=\"x\")\n",
    "        text_widget.configure(yscrollcommand=scrollbar_y.set)\n",
    "        text_widget.configure(xscrollcommand=scrollbar_x.set)\n",
    "        for i, (comment, result) in enumerate(history_list, start=1):\n",
    "            text_widget.insert('end', f\"Bình luận {i}: {comment}\\nKết quả: {result}\\n\\n\")\n",
    "\n",
    "        text_widget.config(state='disabled')\n",
    "\n",
    "    frame_history = ttk.Frame(history_window, borderwidth=0, relief=\"solid\", width=380, height=50)\n",
    "    frame_history.pack(side='top')\n",
    "    frame_history.pack_propagate(False)\n",
    "    ttk.Button(frame_history, text=\"Xóa lịch sử\", command=delete_history).pack(side='left', ipadx=10, ipady=5, fill='both', expand=True)\n",
    "    ttk.Button(frame_history, text=\"Lưu lịch sử\", command=export_history).pack(side='left', ipadx=10, ipady=5, fill='both', expand=True)\n",
    "\n",
    "# Hàm để tải file CSV\n",
    "def upload_file():\n",
    "    file_path = filedialog.askopenfilename(filetypes=[(\"CSV Files\", \"*.csv\")])\n",
    "    if file_path:\n",
    "        global result_df\n",
    "        try:\n",
    "            result_df = classify_comments_from_csv(file_path, my_tokenizer, my_model)\n",
    "            label_configure.config(text=\"File đã được tải lên\")\n",
    "        except Exception as e:\n",
    "            tk.messagebox.showerror(\"Lỗi 404\", f\"Không thể xử lý tệp: {e}\")\n",
    "            label_configure.config(text=\"Vui lòng tải lại.\")\n",
    "    else:\n",
    "        label_configure.config(text=\"Không có tệp được chọn.\")\n",
    "\n",
    "# Hàm để xuất file CSV\n",
    "def export_file():\n",
    "    if 'result_df' in globals():\n",
    "        file_path = filedialog.asksaveasfilename(defaultextension=\".csv\",\n",
    "                                                 filetypes=[(\"CSV Files\", \"*.csv\"),\n",
    "                                                            (\"Text Files\", \"*.txt\"),\n",
    "                                                            (\"Excel Files\", \"*.xlsx\")])\n",
    "        if file_path:\n",
    "            try:\n",
    "                if file_path.endswith('.csv'):\n",
    "                    result_df.to_csv(file_path, index=False)\n",
    "                elif file_path.endswith('.txt'):\n",
    "                    result_df.to_csv(file_path, sep=',', index=False)\n",
    "                elif file_path.endswith('.xlsx'):\n",
    "                    result_df.to_excel(file_path, index=False)\n",
    "                label_configure.config(text=\"Lưu tệp thành công\")\n",
    "            except Exception as e:\n",
    "                tk.messagebox.showerror(\"Lỗi 400\", f\"Không thể xuất tệp: {e}\")\n",
    "                label_configure.config(text=\"Vui lòng thử lại\")\n",
    "    else:\n",
    "        tk.messagebox.showwarning(\"Warning\", \"Không có kết quả để xuất. Vui lòng tải lên và xử lý tệp trước.\")\n",
    "        label_configure.config(text=\"Vui lòng tải lên và xử lý tệp trước.\")\n",
    "\n",
    "# Hàm xóa nội dung văn bản\n",
    "def clear_text():\n",
    "    input_text.delete(\"1.0\", tk.END)\n",
    "    result_label.config(text=\"\")\n",
    "\n",
    "\n",
    "# Tạo cửa sổ chính\n",
    "root = tk.Tk()\n",
    "root.title(\"Phân tích cảm xúc bằng văn bản\")\n",
    "root.iconbitmap(\"img/Logo.ico\")\n",
    "root.geometry(\"600x450\")\n",
    "root.configure(bg=\"#f0f0f0\")\n",
    "root.resizable(width=False, height=False)\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------\n",
    "frame_P1 = ttk.Frame(root, borderwidth=0, relief=\"flat\", width=380, height=50)\n",
    "frame_P1.pack(side='top')\n",
    "frame_P1.pack_propagate(False)\n",
    "\n",
    "# Tiêu đề\n",
    "title_label = ttk.Label(frame_P1, text=\"Phân tích cảm xúc bằng văn bản\", font=(\"Helvetica\", 16))\n",
    "title_label.pack(side='left', padx=5, pady=10)\n",
    "\n",
    "gifImage = \"img/decorate_duck.gif\"\n",
    "openImage = Image.open(gifImage)\n",
    "\n",
    "new_width = 50\n",
    "new_height = 55\n",
    "imageObject = []\n",
    "for frame_num in range(openImage.n_frames):\n",
    "    openImage.seek(frame_num)\n",
    "    resized_frame = openImage.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
    "    imageObject.append(ImageTk.PhotoImage(resized_frame))\n",
    "count = 0\n",
    "\n",
    "def animation(count):\n",
    "    newImage = imageObject[count]\n",
    "    gif_Label.configure(image=newImage)\n",
    "    count += 1\n",
    "    if count == openImage.n_frames:\n",
    "        count = 0\n",
    "    frame_P1.after(50, lambda: animation(count))\n",
    "\n",
    "gif_Label = ttk.Label(frame_P1, image=\"\")\n",
    "gif_Label.pack(side='left', padx=1, pady=1)\n",
    "frame_P1.after(50, lambda: animation(count))\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------\n",
    "frame_P2 = ttk.Frame(root, borderwidth=0, relief=\"flat\", width=600, height=230)\n",
    "frame_P2.pack(side='top')\n",
    "frame_P2.pack_propagate(False)\n",
    "\n",
    "# Hướng dẫn sử dụng\n",
    "help_frame = ttk.Frame(frame_P2, padding=\"10\", borderwidth=2, relief=\"solid\")\n",
    "help_frame.pack(padx=10, pady=5, fill=\"x\")\n",
    "\n",
    "help_label = ttk.Label(help_frame, text=\"Hướng dẫn:\", font=(\"Helvetica\", 14))\n",
    "help_label.pack(pady=5, anchor='w')\n",
    "\n",
    "normal_font = ('Helvetica', 10)\n",
    "help_gt = (\n",
    "    \"- Hãy nhập bất kỳ bình luận nào liên quan đến mua sắm sản phẩm trên các sàn thương mại điện tử.\\n\"\n",
    "    \"- Hệ thống sẽ phân tích và trả về kết quả theo 3 mức độ cảm xúc: NEG, NEU và POS.\\n\"\n",
    "    \"  Chi tiết như sau:\\n\"\n",
    "    \"    ~ NEG: Bình luận mang tính tiêu cực\\n\"\n",
    "    \"    ~ NEU: Bình luận mang tính trung lập\\n\"\n",
    "    \"    ~ POS: Bình luận mang tính tích cực\\n\"\n",
    "    \"* Mức độ chính xác xấp xỉ khoảng 60 - 80%\"\n",
    "    \"- Hãy thử ngay và khám phá xem bình luận của bạn thuộc mức độ nào!\"\n",
    ")\n",
    "\n",
    "help_gioithieu_content = ttk.Label(help_frame, text=help_gt, font=normal_font, wraplength=550)\n",
    "help_gioithieu_content.pack(pady=5, anchor='w')\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------\n",
    "frame_P3 = ttk.Frame(root, borderwidth=0, relief=\"flat\", width=600, height=80)\n",
    "frame_P3.pack(side='top')\n",
    "frame_P3.pack_propagate(False)\n",
    "\n",
    "# Nút để bắt đầu phân tích và xóa\n",
    "text_frame = ttk.Frame(frame_P3,relief=\"flat\", width=450, height=70)\n",
    "text_frame.pack(side=\"left\", padx=10)\n",
    "text_frame.pack_propagate(False)\n",
    "\n",
    "label = ttk.Label(text_frame, text=\"Nhập văn bản:\", font=(\"Helvetica\", 10))\n",
    "label.pack(side='top', anchor='w', padx=5, pady=5)\n",
    "\n",
    "# Text box để nhập văn bản\n",
    "input_text = tk.Text(text_frame, height=1, width=45, font=(\"Helvetica\", 12))\n",
    "input_text.pack(side=\"top\",padx=10, pady=5)\n",
    "\n",
    "# Nút để bắt đầu phân tích và xóa\n",
    "button_frame = ttk.Frame(frame_P3,relief=\"solid\", width=120, height=70)\n",
    "button_frame.pack(side=\"right\", padx=10)\n",
    "button_frame.pack_propagate(False)\n",
    "\n",
    "analyze_button = ttk.Button(button_frame, text=\"Phân tích cảm xúc\", command=on_predict)\n",
    "analyze_button.pack(side='top', ipadx=10, ipady=5, fill='both', expand=True)\n",
    "\n",
    "clear_button = ttk.Button(button_frame, text=\"Xóa\", command=clear_text)\n",
    "clear_button.pack(side='top', ipadx=10, ipady=5, fill='both', expand=True)\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------\n",
    "frame_P4 = ttk.Frame(root, borderwidth=0, relief=\"flat\", width=580, height=40)\n",
    "frame_P4.pack(side='top')\n",
    "frame_P4.pack_propagate(False)\n",
    "\n",
    "result_label = ttk.Label(frame_P4, text=\"Kết quả bình luận:\", font=(\"Helvetica\", 10), wraplength=550, background=\"#f0f0f0\")\n",
    "result_label.pack(side='left',padx=10,pady=10)\n",
    "\n",
    "history_button = ttk.Button(frame_P4, text=\"Lịch sử\", command=show_history)\n",
    "history_button.pack(side='right',padx=5, ipadx=15, ipady=5)\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------\n",
    "frame_P5 = ttk.Frame(root, borderwidth=0, relief=\"solid\", width=580, height=40)\n",
    "frame_P5.pack(side='top',pady=5)\n",
    "frame_P5.pack_propagate(False)\n",
    "\n",
    "frame_button_upload = ttk.Frame(frame_P5, relief=\"flat\", width=320, height=40)\n",
    "frame_button_upload.pack(side=\"left\", padx=5)\n",
    "frame_button_upload.pack_propagate(False)\n",
    "\n",
    "# Tạo nút để tải file CSV\n",
    "upload_button = ttk.Button(frame_button_upload, text=\"Upload CSV\", command=upload_file)\n",
    "upload_button.pack(side=\"left\",padx=5,pady=5,fill='both', expand=True)\n",
    "\n",
    "# Tạo nút để xuất file CSV\n",
    "export_button = ttk.Button(frame_button_upload, text=\"Export CSV\", command=export_file)\n",
    "export_button.pack(side=\"left\",padx=5,pady=5, fill='both', expand=True)\n",
    "\n",
    "label_configure = ttk.Label(frame_P5, text=\"Tải file chứa bình luận\", wraplength=400)\n",
    "label_configure.pack(side='right',padx=5,pady=10, fill='both', expand=True)\n",
    "\n",
    "# Chạy ứng dụng\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X8Rv8MaQReXn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
